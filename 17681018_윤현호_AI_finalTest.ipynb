{"cells": [{"metadata": {"id": "F_YHvLkqjCrc"}, "cell_type": "markdown", "source": "#IBM \uc653\uc2a8 \uc778\uacf5\uc9c0\ub2a5 \uacfc\uc815 \uae30\ub9d0\uace0\uc0ac\n### \uc77c\uc790 : 2021\ub144 6\uc6d4 9\uc77c"}, {"metadata": {"id": "kb_TX0mojY_5"}, "cell_type": "markdown", "source": "##\uc774\ub984 : 'YOUR NAME'"}, {"metadata": {"id": "yIe4C3EfkTDV"}, "cell_type": "markdown", "source": "#\ubb38\uc81c 1 : Gensim\uc744 \uc774\uc6a9\ud55c Word2Vec(15\uc810)\n### \ub2e4\uc74c \ubb38\uc81c\ub294 gensim API\ub97c \uc0ac\uc6a9\ud558\uc5ec Word2Vec\uc73c\ub85c word embedding \ubaa8\ub378\uc744 \uad6c\ud604\ud558\ub294 \ubb38\uc81c\uc785\ub2c8\ub2e4. \uc5ec\ub7ec\ubd84\uc774 \uc2e4\uc2b5\ud558\uc600\ub358 \ub124\uc774\ubc84 \uc601\ud654\ud3c9 \uacfc\uc81c\uc640 \uc720\uc0ac\ud569\ub2c8\ub2e4."}, {"metadata": {"id": "ypnrK__LrPUd"}, "cell_type": "code", "source": "# imports needed and set up logging\nimport gzip\nimport gensim \nimport logging\n\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)", "execution_count": 1, "outputs": []}, {"metadata": {"id": "BLkGIxkmTB3R"}, "cell_type": "markdown", "source": "word embedding \ubaa8\ub378\uc744 \ub9cc\ub4e4 \ub370\uc774\ud130\uc14b\uc744 \ub2e4\uc6b4\ub85c\ub4dc\ubc1b\uc2b5\ub2c8\ub2e4."}, {"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "bIdqVu1KmRNr", "outputId": "dd6d17c3-51e7-477d-ab84-2214a51f0d5d"}, "cell_type": "code", "source": "! wget --quiet --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1W-UgYO0FmmnKYTj2oKaGlsfxCcmIU1y9\" -O reviews_data.txt.gz && rm -rf /tmp/cookies.txt\ndata_file=\"reviews_data.txt.gz\"\n\nwith gzip.open ('reviews_data.txt.gz', 'rb') as f:\n    for i,line in enumerate (f):\n        print(line)\n        break", "execution_count": 2, "outputs": [{"output_type": "stream", "text": "b\"Oct 12 2009 \\tNice trendy hotel location not too bad.\\tI stayed in this hotel for one night. As this is a fairly new place some of the taxi drivers did not know where it was and/or did not want to drive there. Once I have eventually arrived at the hotel, I was very pleasantly surprised with the decor of the lobby/ground floor area. It was very stylish and modern. I found the reception's staff geeting me with 'Aloha' a bit out of place, but I guess they are briefed to say that to keep up the coroporate image.As I have a Starwood Preferred Guest member, I was given a small gift upon-check in. It was only a couple of fridge magnets in a gift box, but nevertheless a nice gesture.My room was nice and roomy, there are tea and coffee facilities in each room and you get two complimentary bottles of water plus some toiletries by 'bliss'.The location is not great. It is at the last metro stop and you then need to take a taxi, but if you are not planning on going to see the historic sites in Beijing, then you will be ok.I chose to have some breakfast in the hotel, which was really tasty and there was a good selection of dishes. There are a couple of computers to use in the communal area, as well as a pool table. There is also a small swimming pool and a gym area.I would definitely stay in this hotel again, but only if I did not plan to travel to central Beijing, as it can take a long time. The location is ok if you plan to do a lot of shopping, as there is a big shopping centre just few minutes away from the hotel and there are plenty of eating options around, including restaurants that serve a dog meat!\\t\\r\\n\"\n", "name": "stdout"}]}, {"metadata": {"id": "1T6Z-46STIQV"}, "cell_type": "markdown", "source": "\ub2e4\uc6b4\ub85c\ub4dc \ubc1b\uc740 \ud30c\uc77c\uc744 \ud55c \uc904 \uc529 \uc77d\uc2b5\ub2c8\ub2e4."}, {"metadata": {"id": "GVe9Mgutmda-"}, "cell_type": "code", "source": "def read_input(input_file):\n    \"\"\"This method reads the input file which is in gzip format\"\"\"\n    \n    logging.info(\"reading file {0}...this may take a while\".format(input_file))\n    \n    with gzip.open (input_file, 'rb') as f:\n        for i, line in enumerate (f): \n\n            if (i%10000==0):\n                logging.info (\"read {0} reviews\".format (i))\n            # do some pre-processing and return a list of words for each review text\n            yield gensim.utils.simple_preprocess (line)\n\n# read the tokenized reviews into a list\n# each review item becomes a serries of words\n# so this becomes a list of lists\ndocuments = list (read_input (data_file))\nlogging.info (\"Done reading data file\")    ", "execution_count": null, "outputs": []}, {"metadata": {"id": "FkoZ1y6eNQlp"}, "cell_type": "markdown", "source": "##\ubb38\uc81c1-1. Word2Vec API\ub97c \uc0ac\uc6a9\ud558\uc5ec \ub2e4\uc6b4\ub85c\ub4dc\ubc1b\uc740 \ud30c\uc77c\uc5d0 \ub300\ud55c word embedding \ubaa8\ub378\uc744 \ud2b8\ub808\uc774\ub2dd\uc2dc\ud0a4\uc2ed\uc2dc\uc624.(5\uc810)\nWord2Vec\uc758 \ud30c\ub77c\ubbf8\ud130\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uc801\uc6a9\ud558\uc2ed\uc2dc\uc624. \nsize=150, window=10, min_count=2, workers=10, iter=1\n\n\ud2b8\ub808\uc774\ub2dd \uc2dc\ud0a4\ub294\ub370 2\ubd84 \uac00\ub7c9 \uc18c\uc694\ub429\ub2c8\ub2e4."}, {"metadata": {"id": "i55O0qECmiLm"}, "cell_type": "code", "source": "%time model = word2vec.Word2Vec(Data, size=150, window=10, min_count=2, workers=10, iter=1),\ntime model.save(\\\"Models.model\\\"),\nend = time model()\n\n", "execution_count": null, "outputs": []}, {"metadata": {"id": "KMiqYJmLnsJw"}, "cell_type": "markdown", "source": "##\ubb38\uc81c1-2. gensim\uc758 API\ub97c \uc0ac\uc6a9\ud558\uc5ec 'dirty'\uc758 \ub3d9\uc758\uc5b4\ub4e4\uc744 \ucc3e\uc544\ub0b4\uc2ed\uc2dc\uc624. \n\uc544\ub798 \ub9c1\ud06c\uc758 \ub124\uc774\ubc84 \uc601\ud654\ud3c9 \ucf54\ub4dc\ub97c \ucc38\uc870\ud558\uc2ed\uc2dc\uc624.\n\n\ucc38\uc870 : https://colab.research.google.com/drive/1kY7mpCXo_5RDB-WpTO_4bARDFaZB_Vyi#scrollTo=45SmN7RK7hJb"}, {"metadata": {"id": "1Lne2vO8mjfn"}, "cell_type": "code", "source": "# dirty\uc640 \uac00\uc7a5 similar\ud55c \ub2e8\uc5b4\ub4e4\uc744 \ucd9c\ub825\ud558\uc2ed\uc2dc\uc624.\ntime model = word2vec,Word2Vec.load(\"Models.model\")\nprint(\"****dirty****\")\nprint(model.wv.most_similar(positive=[\"dirty\"]))", "execution_count": null, "outputs": []}, {"metadata": {"id": "GV48dmBwQik4"}, "cell_type": "markdown", "source": "##\ubb38\uc81c1-3. gensim\uc758 API\ub97c \uc0ac\uc6a9\ud558\uc5ec 'dirty'\uc640 'unclean'\uac04\uc758 \ucf54\uc0ac\uc778 \uc720\uc0ac\ub3c4(cosine similarity)\ub97c \ucd9c\ub825\ud558\uc2ed\uc2dc\uc624.\n\ub2e4\uc74c \ub9c1\ud06c\uc758 similarity \uacc4\uc0b0 \uc608\uc81c\ub97c \ucc38\uace0\ud558\uc2ed\uc2dc\uc624.\n\n\ucc38\uace0 https://radimrehurek.com/gensim_3.8.3/auto_examples/tutorials/run_word2vec.html#sphx-glr-auto-examples-tutorials-run-word2vec-py"}, {"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "8U-RaLt7msul", "outputId": "a47df975-031c-471a-cfbe-9d2d36d38d21"}, "cell_type": "code", "source": "# similarity between 'dirty' and 'unclean'\nimport gensim.downloader as api\nwv = api.load('Models.model'')\nfor i, word in enumerate(wv.vocab):\n    if i == 10:\n        break\n    print(word)\nvec_king = wv['dirty', 'unclean']\ntry:\n    vec_cameroon = wv['dirty', 'unclean']\nexcept KeyError:\n    print(\"The word 'dirty', 'unclean' does not appear in this model\")    \npairs = [\n            ('dirty', 'unclean')\n]\nfor w1, w2 in pairs:\n    print('%r\\t%r\\t%.2f' % (w1, w2, wv.similarity(w1, w2)))              ", "execution_count": 7, "outputs": [{"output_type": "execute_result", "data": {"text/plain": "0.8418293"}, "metadata": {"tags": []}, "execution_count": 7}]}, {"metadata": {"id": "kU9JKdNvrP2w"}, "cell_type": "markdown", "source": "#\ubb38\uc81c 2: RNN\uc744 \uc774\uc6a9\ud55c \ud55c\uad6d\uc5b4 \ubb38\uc7a5 \uc0dd\uc131\uae30 \ucf54\ub529(15\uc810)\n### \ub2e4\uc74c \ucf54\ub4dc \uc911 your_code_here \ubd80\ubd84\uc744 \uc644\uc131\ud558\uc2ed\uc2dc\uc624. P10-1\uc758 \"\uae00\uc4f0\ub294 \uc778\uacf5\uc9c0\ub2a52 \uc2e4\uc2b5\"\uc5d0 \uc0ac\uc6a9\ub41c \ucf54\ub4dc\ub97c \ucc38\uace0\ud558\uc2ed\uc2dc\uc624."}, {"metadata": {"id": "zZDuqyL_rmYG"}, "cell_type": "code", "source": "from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport numpy as np\nfrom tensorflow.keras.utils import to_categorical", "execution_count": null, "outputs": []}, {"metadata": {"id": "MkcsdQrnVAY2"}, "cell_type": "markdown", "source": "\ud559\uc2b5\ud560 \uc785\ub825 \ubb38\uc7a5\uc744 \uc815\uc758\ud569\ub2c8\ub2e4."}, {"metadata": {"id": "32jTHO15VAzN"}, "cell_type": "code", "source": "text=\"\"\"\ud2b9\ud788 \ucd5c\uadfc \uc218\ub3c4\uad8c\uc5d0\uc11c\ub294 \uad50\ud68c \uc18c\ubaa8\uc784 \ucc38\uc11d\uc790\uc5d0 \uc774\uc5b4 \uc774\ub4e4 \uac00\uc871\uacfc \uc9c0\uc778\uc73c\ub85c \ubc88\uc9c0\ub294 2\ucc28 \uac10\uc5fc \uc0ac\ub840\uac00 \uc99d\uac00\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc911\uc559\uc7ac\ub09c\uc548\uc804\ub300\ucc45\ubcf8\ubd80\ub294 \uc624\ub298\uae4c\uc9c0 \uc218\ub3c4\uad8c \uad50\ud68c\uc640 \uad00\ub828\ud55c \ucf54\ub85c\ub09819 \ud655\uc9c4\uc790\ub294 \ucd1d 63\uba85\uc774\ub77c\uace0 \ubc1d\ud614\uc73c\uba70  2\ucc28 \uac10\uc5fc\uc790\ub294 33\uba85 \uc774\ub77c\uace0 \uc804\ud588\uc2b5\ub2c8\ub2e4. \"\"\"", "execution_count": null, "outputs": []}, {"metadata": {"id": "V9R3xTkxT1S4"}, "cell_type": "markdown", "source": "\ub370\uc774\ud130 \uc804\ucc98\ub9ac\ub97c \uc218\ud589\ud569\ub2c8\ub2e4."}, {"metadata": {"id": "HTQUpKieSpdw"}, "cell_type": "code", "source": "t = Tokenizer()\nt.fit_on_texts([text])\nvocab_size = len(t.word_index) + 1\nprint('\ub2e8\uc5b4 \uc9d1\ud569\uc758 \ud06c\uae30 : %d' % vocab_size)\n\n\nprint(t.word_index)\n\nsequences = list()\nfor line in text.split('\\n'): \n    encoded = t.texts_to_sequences([line])[0]\n    for i in range(1, len(encoded)):\n        sequence = encoded[:i+1]\n        sequences.append(sequence)\n\nprint('\ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud560 \uc0d8\ud50c\uc758 \uac1c\uc218: %d' % len(sequences))\n\n\nmax_len=max(len(l) for l in sequences) \n\nsequences = pad_sequences(sequences, maxlen=max_len, padding='pre')\n\nsequences = np.array(sequences)\nX = sequences[:,:-1]\ny = sequences[:,-1]", "execution_count": null, "outputs": []}, {"metadata": {"id": "A94dnqA8T8qg"}, "cell_type": "markdown", "source": "Word Embedding \ubaa8\ub378\uc744 Keras\ub85c \uc815\uc758\ud558\uace0 \ud2b8\ub808\uc774\ub2dd\uc744 \uc218\ud589\ud569\ub2c8\ub2e4."}, {"metadata": {"id": "I3xzFR9gTpGE"}, "cell_type": "code", "source": "from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding, Dense, SimpleRNN\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, 10, input_length=max_len-1))  \nmodel.add(SimpleRNN(32))\nmodel.add(Dense(vocab_size, activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.fit(X, y, epochs=120, verbose=2)", "execution_count": null, "outputs": []}, {"metadata": {"id": "Is_TW5rxUSla"}, "cell_type": "markdown", "source": "##\ubb38\uc81c2-1~3. \uc774 \uc140\uc5d0\uc11c 3 \uacf3\uc758 your_code_here\ub97c \ucc3e\uc544\uc11c \ucf54\ub4dc\ub97c \uc644\uc131\ud558\uc2ed\uc2dc\uc624.(15\uc810)"}, {"metadata": {"id": "FLQpmYxSTqN-"}, "cell_type": "code", "source": "# \uc774 \ube14\ub85d\uc5d0\uc11c yoou_code_here \ubd80\ubd84\uc744 \uc644\uc131\ud558\uc2dc\uc624\ndef sentence_generation(model, t, current_word, n): # model = \ubaa8\ub378, t = \ud1a0\ud06c\ub098\uc774\uc800, current_word = \ud604\uc7ac \ub2e8\uc5b4, n = \ubc18\ubcf5\ud560 \ud69f\uc218\n    init_word = current_word # \ucc98\uc74c \ub4e4\uc5b4\uc628 \ub2e8\uc5b4\ub3c4 \ub9c8\uc9c0\ub9c9\uc5d0 \uac19\uc774 \ucd9c\ub825\ud558\uae30\uc704\ud574 \uc800\uc7a5\n    sentence = ''\n    for _ in range(n): # n\ubc88 \ubc18\ubcf5\n        encoded = t.texts_to_sequences([current_word])[0] # \ud604\uc7ac \ub2e8\uc5b4\uc5d0 \ub300\ud55c \uc815\uc218 \uc778\ucf54\ub529\n        encoded = pad_sequences([encoded], maxlen=30, padding='pre') # \ub370\uc774\ud130\uc5d0 \ub300\ud55c \ud328\ub529\n        result = np.argmax(model.predict(encoded), axis=-1)\n        for word, index in t.word_index.items(): \n            if index == result: # \ub9cc\uc57d \uc608\uce21\ud55c \ub2e8\uc5b4\uc758 \uc778\ub371\uc2a4 \uac12\uc774 \ub3d9\uc77c\ud55c \ub2e8\uc5b4\uac00 \uc788\ub2e4\uba74\n                break # \ud574\ub2f9 \ub2e8\uc5b4\uac00 \uc608\uce21 \ub2e8\uc5b4\uc774\ubbc0\ub85c break\n        # \uc544\ub798 \ubd80\ubd84\uc758 \ucf54\ub529\uc744 \uc644\uc131\ud558\uc2ed\uc2dc\uc624        \n        current_word = your_code_here # \ud604\uc7ac \ub2e8\uc5b4 + ' ' + \uc608\uce21\ud55c \ub2e8\uc5b4\ub97c current_word\ub85c \uc800\uc7a5(5\uc810)\n        sentence = your_code_here  # \uc608\uce21\ud55c \ub2e8\uc5b4\ub97c sentence\uc5d0 append(5\uc810)\n     \n    sentence = your_code_here # \ucd08\uae30\uc758 \ud14d\uc2a4\ud2b8\uc640 for loop\uc5d0\uc11c \uc0dd\uc131\ub41c \ud14d\uc2a4\ud2b8\ub97c concatenate(5\uc810)\n    return sentence\n\nprint(sentence_generation(model, t, '\ud2b9\ud788', 10))", "execution_count": null, "outputs": []}, {"metadata": {"id": "nXAXa6AXrqbU"}, "cell_type": "markdown", "source": "#\ubb38\uc81c3 : Keras tuner"}, {"metadata": {"id": "DVsWQo8DVNsL"}, "cell_type": "code", "source": "from tensorflow.keras import applications\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import backend\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras import optimizers", "execution_count": null, "outputs": []}, {"metadata": {"id": "VLIiyRQ4NJ6W"}, "cell_type": "code", "source": "!pip install -q -U keras-tuner\nimport kerastuner as kt", "execution_count": null, "outputs": []}, {"metadata": {"id": "c73LL_nXQ2de"}, "cell_type": "code", "source": "# load CIFAR-10 dataset\n(img_train, y_train), (img_test, y_test) = keras.datasets.cifar10.load_data()", "execution_count": null, "outputs": []}, {"metadata": {"id": "hyi1MLXnSg7q"}, "cell_type": "code", "source": "from tensorflow.keras import backend\n# Normalize pixel values between 0 and 1\nx_train = img_train.astype('float32') / 255.0\nx_test = img_test.astype('float32') / 255.0\n\nimg_rows, img_cols = 32,32\nif backend.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 3, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 3, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 3)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 3)\n    input_shape = (img_rows, img_cols, 3)", "execution_count": null, "outputs": []}, {"metadata": {"id": "NEqPtul5i5mK"}, "cell_type": "markdown", "source": "##\ubb38\uc81c3-1. \uc804\uc774\ud559\uc2b5 : VGG16\uc744 pre-trained \ubaa8\ub378\ub85c  \uc0ac\uc6a9\ud560 \uc218 \uc788\ub3c4\ub85d \ub2e4\uc74c \uc140\uc758 \ucf54\ub4dc\ub97c \uc644\uc131\ud558\uc2ed\uc2dc\uc624.(5\uc810)\n\uc544\ub798\uc758 \uc140\uc5d0\uc11c\ub294 VGG16\uc758 Pre-trained \ubaa8\ub378\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774 \ubc29\ubc95\uc5d0 \ub300\ud558\uc5ec\ub294 P11-2 VGG16\uc2e4\uc2b5\uc744 \ucc38\uace0\ud558\uc2ed\uc2dc\uc624.\n### \ucc38\uace0 https://keras.io/ko/applications/#vgg16"}, {"metadata": {"id": "Ot6sWWoVK-Nb"}, "cell_type": "code", "source": "# define the model\ndef model_builder(hp):\n    base_model = applications.VGG16(weights='imagenet', pooling='avg', include_top='false') # your_code_here\uc5d0 \uc801\uc808\ud55c \uac12\uc744 \ub123\uc73c\uc2ed\uc2dc\uc624.\n    inputs = tf.keras.Input(shape=(32, 32, 3))\n    x = base_model(inputs)\n    for i in range(hp.Int('num_layers', min_value=2, max_value=5, step=1)):\n        x = Dense(units=hp.Int('units' + str(i), min_value=32, max_value=128, step=32), activation='relu')(x)\n    x = Dense(10, activation='softmax')(x) \n    model = tf.keras.Model(inputs=inputs, outputs=x)\n    model.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n                  loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    return model", "execution_count": null, "outputs": []}, {"metadata": {"id": "2qLUZsnImR8p"}, "cell_type": "markdown", "source": "##\ubb38\uc81c3-2. \uc544\ub798 \uc140\uc5d0 tuner \uc624\ube0c\uc81d\ud2b8 \uc0dd\uc131\ud558\ub294 \ucf54\ub4dc\ub97c \uc644\uc131\ud558\uc2dc\uc624. \uc774 \ud29c\ub108\ub294 RandomSearch\ub97c \uc0ac\uc6a9\ud558\uace0 \uadf8\ub97c \uc704\ud55c \uc778\uc218\uac12\uc740 \ub2e4\uc74c\uc744 \uc0ac\uc6a9\ud558\uc2dc\uc624.(5\uc810)\n\n\n    objective='val_accuracy',\n    max_trials=3,\n    executions_per_trial=1,\n    directory='tuning_dir',\n    project_name='ktuner'"}, {"metadata": {"id": "D_GUsErT17u1"}, "cell_type": "code", "source": "tuner = kt.Hyperband(model_builder,\n                    objective='val_accuracy',\n                     max_trials=3,\n                     executions_per_trial=1,\n                     directory='tuning_dir',\n                     project_name='ktuner')", "execution_count": null, "outputs": []}, {"metadata": {"id": "S2g2WQgc5tdo"}, "cell_type": "code", "source": "#Run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above.\n%time tuner.search(x_train, y_train, epochs = 1, validation_data = (x_test, y_test))", "execution_count": null, "outputs": []}, {"metadata": {"id": "TlTbmaKYStFL"}, "cell_type": "markdown", "source": "##\ubb38\uc81c3-3. \ucd5c\uc801\uc758 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub4e4\uc758 \uc870\ud569\uc744 print\ubb38\uc73c\ub85c \ucd9c\ub825\ud558\uc2dc\uc624. (5\uc810)"}, {"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "kCjyCGlyOgEe", "outputId": "d82003f3-9d77-466e-e51d-3db1a8074163"}, "cell_type": "code", "source": "print('num_layers': ('numlayers'), 'units0': ('units0'), 'units1': ('units1'), 'learning_rate': ('leraning_rate'), 'units2': ('units2'), 'units3': ('units3')) #\uc544\ub798\uc5d0 \uc608\uc2dc\ud55c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub4e4\uc744 \ucd9c\ub825\ud558\uc2dc\uc624.", "execution_count": null, "outputs": [{"output_type": "stream", "text": "{'num_layers': 4, 'units0': 96, 'units1': 128, 'learning_rate': 0.001, 'units2': 32, 'units3': 32}\n", "name": "stdout"}]}, {"metadata": {"id": "a2_mC3_7Se80"}, "cell_type": "code", "source": "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)", "execution_count": null, "outputs": []}, {"metadata": {"colab": {"base_uri": "https://localhost:8080/"}, "id": "1yqKdFCnR_kJ", "outputId": "b007ffff-508f-48fd-bda5-fa1896465413", "scrolled": true}, "cell_type": "code", "source": "tuner.get_best_models()[0].summary()", "execution_count": null, "outputs": [{"output_type": "stream", "text": "Model: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\ninput_2 (InputLayer)         [(None, 32, 32, 3)]       0         \n_________________________________________________________________\nvgg16 (Functional)           (None, 512)               14714688  \n_________________________________________________________________\ndense (Dense)                (None, 96)                49248     \n_________________________________________________________________\ndense_1 (Dense)              (None, 128)               12416     \n_________________________________________________________________\ndense_2 (Dense)              (None, 32)                4128      \n_________________________________________________________________\ndense_3 (Dense)              (None, 32)                1056      \n_________________________________________________________________\ndense_4 (Dense)              (None, 10)                330       \n=================================================================\nTotal params: 14,781,866\nTrainable params: 14,781,866\nNon-trainable params: 0\n_________________________________________________________________\n", "name": "stdout"}]}, {"metadata": {"id": "LNlFFkFOijfn"}, "cell_type": "markdown", "source": "#\ucf54\ub529 \ud14c\uc2a4\ud2b8\ub97c \uc644\ub8cc\ud558\uc600\uc2b5\ub2c8\ub2e4. 1\ud559\uae30-\uae30\ub9d0\uace0\uc0ac-yourname.ipynb\ub85c \ub2e4\uc6b4\ub85c\ub4dc\ubc1b\uc544\uc11c LMS\uc640 \uae43\ud5d9\uc5d0 \uacfc\uc81c\ubb3c\ub85c \uc81c\ucd9c\ud574\uc8fc\uc2ed\uc2dc\uc624.\n#\uc218\uace0 \ud558\uc600\uc2b5\ub2c8\ub2e4!"}], "metadata": {"accelerator": "GPU", "colab": {"name": "watsonAI-finalTest.ipynb", "provenance": [], "collapsed_sections": []}, "kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}