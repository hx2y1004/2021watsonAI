{"cells": [{"metadata": {"scrolled": true}, "cell_type": "code", "source": "\nimport os, types\nimport pandas as pd\nfrom botocore.client import Config\nimport ibm_boto3\n\ndef __iter__(self): return 0\n\n# @hidden_cell\n# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n# You might want to remove those credentials before you share the notebook.\n\nif os.environ.get('RUNTIME_ENV_LOCATION_TYPE') == 'external':\n    endpoint_67a46cf1bad54548ab3788753d89e533 = 'https://s3.ap-geo.objectstorage.softlayer.net'\nelse:\n    endpoint_67a46cf1bad54548ab3788753d89e533 = 'https://s3.ap-geo.objectstorage.service.networklayer.com'\n\nclient_67a46cf1bad54548ab3788753d89e533 = ibm_boto3.client(service_name='s3',\n    ibm_api_key_id='Ex34odSXL2FIhrCkqMlSHR9z1UOyW38IDnYzcXkbBucp',\n    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n    config=Config(signature_version='oauth'),\n    endpoint_url=endpoint_67a46cf1bad54548ab3788753d89e533)\n\nbody = client_67a46cf1bad54548ab3788753d89e533.get_object(Bucket='iris-donotdelete-pr-f7p2ui7zmtjr49',Key='Iris.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_1 = pd.read_csv(body)\ndf_data_1.head(10)\n", "execution_count": 44, "outputs": [{"output_type": "error", "ename": "ParserError", "evalue": "Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n", "traceback": ["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m", "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)", "\u001b[0;32m<ipython-input-44-a3b77a3f1493>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__iter__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMethodType\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0m__iter__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbody\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mdf_data_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0mdf_data_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    674\u001b[0m         )\n\u001b[1;32m    675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1133\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m         \u001b[0;31m# May alter columns / col_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32m/opt/conda/envs/Python-3.7-main/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   2035\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2036\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2037\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2038\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2039\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n", "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[0;34m()\u001b[0m\n", "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[0;34m()\u001b[0m\n", "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n", "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n", "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n", "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 1 fields in line 4, saw 2\n"]}]}, {"metadata": {}, "cell_type": "code", "source": "import tensorflow as tf ", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#\ud488\uc885 column\uc744 one-hot-encode\niris_data_one_hot_encoded = pd.get_dummies(df_data_1)\niris_data_one_hot_encoded.head(5)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Visualize the Data"}, {"metadata": {}, "cell_type": "code", "source": "import matplotlib.pyplot as plt\n\nnames = {'Iris-setosa','Iris-versicolor','Iris-virginica'}\nfeature_names = ['SepalLengthCm','SepalWidthCm','PetalLengthCm','PetalWidthCm']\nlabels=df_data_1.iloc[:,5]", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# Visualize the data sets\nplt.figure(figsize=(16, 6))\nplt.subplot(1, 2, 1)\nfor target, target_name in enumerate(names):\n    X_plot = df_data_1[labels== target_name]\n    plt.plot(X_plot.iloc[:, 1], X_plot.iloc[:, 2], linestyle='none', marker='o', label=target_name)\nplt.xlabel(feature_names[0])\nplt.ylabel(feature_names[1])\nplt.axis('equal')\nplt.legend();\n\n\nplt.subplot(1, 2, 2)\nfor target, target_name in enumerate(names):\n    X_plot = df_data_1[labels== target_name]\n    plt.plot(X_plot.iloc[:, 3], X_plot.iloc[:, 4], linestyle='none', marker='o', label=target_name)\nplt.xlabel(feature_names[2])\nplt.ylabel(feature_names[3])\nplt.axis('equal')\nplt.legend();", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#\uc804\uccb4 \ub370\uc774\ud130\ub97c 80%\uc740 \ud2b8\ub808\uc774\ub2dd 20% \ud14c\uc2a4\ud2b8\ub85c \ucabc\uac2c\niris_train_data = iris_data_one_hot_encoded.sample(frac=0.8, random_state=200)\niris_test_data = iris_data_one_hot_encoded.drop(iris_train_data.index)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#input\uc740 \uaf43\uc78e\uc758 \ub108\ube44\uc640\uae38\uc774, \uaf43\ubc1b\uce68\uc758 \ub108\ube44\uc640\uae38\uc774\n#output\uc740 \uc138\uac1c\uc911 \ud558\ub098\ub85c\niris_train_input_data = iris_train_data.filter(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])\niris_train_label_data = iris_train_data.filter(['Species_Iris-setosa', 'Species_Iris-versicolor', 'Species_Iris-virginica'])\niris_test_input_data = iris_test_data.filter(['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm'])\niris_test_label_data = iris_test_data.filter(['Species_Iris-setosa', 'Species_Iris-versicolor', 'Species_Iris-virginica'])\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#x\ub294 input\uac12\uc744 \uc704\ud55c placeholder\n#w\ub294 \uac00\uc911\uce58\n#b\ub294 \ud3b8\ucc28\n#y\ub294 \ud2b8\ub808\uc774\ub2dd\ud574\uc11c \ub098\uc628 \uacb0\uacfc(\uac00\uc124)\n#y_\ub294 \uc9c4\uc9dc \uacb0\uacfc\uac12\nx = tf.placeholder(tf.float32,[None, 4])\nW = tf.Variable(tf.zeros([4, 3]))\nb = tf.Variable(tf.zeros([3]))\ny = tf.nn.softmax(tf.matmul(x, W) + b)\ny_ = tf.placeholder(tf.float32, [None, 3])", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#cross_entropy\ub97c cost\ud568\uc218\ub85c\ncross_entropy  = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_, logits=tf.matmul(x,W)+b))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#cost\ub97c \ucd5c\uc18c\ud654\ntrain_step = tf.train.GradientDescentOptimizer(0.05).minimize(cross_entropy)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "sess = tf.InteractiveSession()", "execution_count": null, "outputs": []}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "#30,000\ubc88 \ud559\uc2b5\n#tf.global_variables_initializer().run()\nepoch_history = []\nloss_history = []\n\nsess.run(tf.global_variables_initializer())\nfor _ in range(30000):\n    #Usually send batches to the training step. But since the dataset is small sending all\n    l,a=sess.run([cross_entropy,train_step], feed_dict={x: iris_train_input_data, y_: iris_train_label_data})\n    epoch_history.append(_)\n    loss_history.append(l)\n\nprint(\"trained\")", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Crossentropy vs. epoch"}, {"metadata": {}, "cell_type": "code", "source": "#Draw Cross Entropy Graph\nplt.xlabel('Epochs')\nplt.ylabel('crossentropy')\n\n\n# Show the cross_entropy\nplt.plot(epoch_history, loss_history)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n#\uc815\ud655\ub3c4\naccuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\nprint('Accuracy : ', sess.run(accuracy, feed_dict={x: iris_test_input_data, y_: iris_test_label_data}))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "#\ucd94\ub860\npredict_x = np.array([[6.9,3.1,5.4,2.1]])\nprediction=tf.argmax(y,1)\nprint('\uc608\uce21\uac12:', sess.run(prediction, feed_dict={x: predict_x}))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "predict_x = np.array([[5.1,3.3,1.7,0.5]])\nprediction=tf.argmax(y,1)\nprint('\uc608\uce21\uac12:', sess.run(prediction, feed_dict={x: predict_x}))", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## ROC Curve"}, {"metadata": {"scrolled": false}, "cell_type": "code", "source": "# quality of the neural net using ROC curve and AUC\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import label_binarize\n\n#test\uc778\ud48b\uc758 \uc608\uce21\uac12\ntest_prob=sess.run(tf.nn.softmax(logits=tf.matmul(x,W)+b), feed_dict={x: iris_test_input_data})\n#print(test_prob.ravel())\n\n#test\uc778\ud48b\uc758 label\ntest_label=sess.run(tf.argmax(y_,1), feed_dict={y_: iris_test_label_data})\n#print(test_label)\n\n#test\uc778\ud48b\uc758 binarized\nbi_test_label = label_binarize(test_label, classes=[0, 1, 2])\n#print(bi_test_label)\n", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "# send the actual dependent variable classifications for param 1, \n# and the confidences of the true classification for param 2.\n#roc_curve\ub294 multiclass&multi_Indicater\uc758 \uc785\ub825\uc774 \ub418\uc9c0 \uc54a\uc73c\ubbc0\ub85c ravel()\uc744 \ud1b5\ud574 1\ucc28\uc6d0\ubc30\uc5f4\ub85c \ud3bc\uce68\nfpr,tpr,thr= roc_curve(bi_test_label.ravel(), test_prob.ravel())\nAUC = auc(fpr,tpr)\nplt.plot(fpr, tpr,label='%s(area = %0.2f)' %('Model',AUC))\n\nplt.legend(loc=\"lower right\")\n#plt.xlabel('\uc704\uc591\uc131\ub960(Fall-Out)')\n#plt.ylabel('\uc7ac\ud604\ub960(Recall)')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Curve')\nplt.show()", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "markdown", "source": "## Confusion Matrix"}, {"metadata": {}, "cell_type": "code", "source": "\nbody = client_67a46cf1bad54548ab3788753d89e533.get_object(Bucket='iris-donotdelete-pr-f7p2ui7zmtjr49',Key='Iris.csv')['Body']\n# add missing __iter__ method, so pandas accepts body as file-like object\nif not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n\ndf_data_1 = pd.read_csv(body)\ndf_data_1.head()\n#confusion_matrix\nfrom sklearn.metrics import confusion_matrix\n\nprint('label : ',test_label)\ntest_pred=sess.run(tf.argmax(y,1), feed_dict={x: iris_test_input_data})\nprint('predict: ',test_pred)\n\nconfM=confusion_matrix(test_label,test_pred)\nprint(confM)", "execution_count": null, "outputs": []}, {"metadata": {}, "cell_type": "code", "source": "", "execution_count": null, "outputs": []}], "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3.7", "language": "python"}, "language_info": {"name": "python", "version": "3.7.10", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}}, "nbformat": 4, "nbformat_minor": 1}